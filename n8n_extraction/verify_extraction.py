#!/usr/bin/env python
"""
Script de v√©rification des fichiers d√©j√† extraits pour √©viter les doublons
Analyse les fichiers existants et compare avec les URLs √† traiter
"""

import os
import re
import csv # Ajout de l'importation du module csv
from pathlib import Path
from urllib.parse import urlparse
import json
from datetime import datetime

class ExtractionVerifier:
    def __init__(self, output_dir='./output'):
        self.output_dir = Path(output_dir)
        self.existing_files = {}
        self.url_mapping = {}
        
    def slugify(self, text):
        """Convertit un texte en slug utilisable comme nom de fichier"""
        text = text.lower()
        text = re.sub(r'[^\w\s-]', '', text)
        text = re.sub(r'[-\s]+', '-', text)
        return text.strip('-')
    
    def url_to_filename(self, url):
        """Convertit une URL en nom de fichier attendu"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        if not path:
            return "index"
        
        # Remplacer les caract√®res sp√©ciaux
        filename = self.slugify(path.replace('/', '_'))
        return filename
    
    def scan_existing_files(self):
        """Scan tous les fichiers existants dans le r√©pertoire de sortie"""
        print("üîç Scan des fichiers existants...")
        
        if not self.output_dir.exists():
            print(f"‚ùå R√©pertoire de sortie n'existe pas: {self.output_dir}")
            return
        
        total_files = 0
        
        for category_dir in self.output_dir.iterdir():
            if category_dir.is_dir():
                category_name = category_dir.name
                self.existing_files[category_name] = []
                
                txt_files = list(category_dir.glob("*.txt"))
                
                for txt_file in txt_files:
                    # Lire l'en-t√™te du fichier pour extraire l'URL source
                    try:
                        with open(txt_file, 'r', encoding='utf-8') as f:
                            first_line = f.readline().strip()
                            if first_line.startswith('# https://'):
                                source_url = first_line[2:].strip()
                                self.existing_files[category_name].append({
                                    'filename': txt_file.name,
                                    'filepath': str(txt_file),
                                    'source_url': source_url,
                                    'size': txt_file.stat().st_size,
                                    'modified': datetime.fromtimestamp(txt_file.stat().st_mtime)
                                })
                                self.url_mapping[source_url] = str(txt_file)
                                total_files += 1
                    except Exception as e:
                        print(f"‚ùå Erreur lecture fichier {txt_file}: {e}")
                
                if self.existing_files[category_name]:
                    print(f"üìÅ {category_name}: {len(self.existing_files[category_name])} fichiers")
        
        print(f"üìä Total de fichiers existants: {total_files}")
        return total_files
    
    def check_url_already_extracted(self, url):
        """V√©rifie si une URL a d√©j√† √©t√© extraite"""
        return url in self.url_mapping
    
    def get_expected_filename(self, url, category):
        """Retourne le nom de fichier attendu pour une URL"""
        filename = self.url_to_filename(url)
        expected_path = self.output_dir / category / f"{filename}.txt"
        return expected_path
    
    def verify_extraction_completeness(self, urls_to_extract):
        """V√©rifie la compl√©tude de l'extraction"""
        print("\nüîç V√©rification de la compl√©tude de l'extraction...")
        
        stats = {
            'total_urls': len(urls_to_extract),
            'already_extracted': 0,
            'missing': 0,
            'to_extract': [],
            'missing_urls': []
        }
        
        for category, urls in urls_to_extract.items():
            print(f"\nüìÅ Cat√©gorie: {category}")
            
            category_stats = {
                'total': len(urls),
                'extracted': 0,
                'missing': 0
            }
            
            for url in urls:
                if self.check_url_already_extracted(url):
                    category_stats['extracted'] += 1
                    stats['already_extracted'] += 1
                else:
                    expected_file = self.get_expected_filename(url, category)
                    
                    if expected_file.exists():
                        category_stats['extracted'] += 1
                        stats['already_extracted'] += 1
                    else:
                        category_stats['missing'] += 1
                        stats['missing'] += 1
                        stats['to_extract'].append((category, url))
                        stats['missing_urls'].append(url)
            
            print(f"   ‚úÖ Extraites: {category_stats['extracted']}")
            print(f"   ‚ùå Manquantes: {category_stats['missing']}")
            print(f"   üìä Taux: {category_stats['extracted']/category_stats['total']*100:.1f}%")
        
        return stats
    
    def generate_extraction_report(self, stats):
        """G√©n√®re un rapport d'extraction"""
        print("\n" + "="*60)
        print("üìä RAPPORT D'EXTRACTION")
        print("="*60)
        
        print(f"URLs totales √† traiter: {stats['total_urls']}")
        print(f"URLs d√©j√† extraites: {stats['already_extracted']}")
        print(f"URLs manquantes: {stats['missing']}")
        
        if stats['total_urls'] > 0:
            completion_rate = (stats['already_extracted'] / stats['total_urls']) * 100
            print(f"Taux de compl√©tude: {completion_rate:.1f}%")
        
        if stats['missing'] > 0:
            print(f"\n‚ùå URLs manquantes ({stats['missing']}):")
            for i, (category, url) in enumerate(stats['to_extract'][:10], 1):
                print(f"   {i}. [{category}] {url}")
            
            if len(stats['to_extract']) > 10:
                print(f"   ... et {len(stats['to_extract']) - 10} autres URLs")
        
        # Sauvegarder le rapport
        report_file = self.output_dir / 'extraction_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'stats': stats,
                'missing_urls': stats['missing_urls']
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nüìÑ Rapport sauvegard√©: {report_file}")
    
    def clean_empty_files(self):
        """Nettoie les fichiers vides ou corrompus"""
        print("\nüßπ Nettoyage des fichiers vides/corrompus...")
        
        cleaned_count = 0
        
        for category_dir in self.output_dir.iterdir():
            if category_dir.is_dir():
                for txt_file in category_dir.glob("*.txt"):
                    try:
                        if txt_file.stat().st_size == 0:
                            print(f"üóëÔ∏è  Suppression fichier vide: {txt_file}")
                            txt_file.unlink()
                            cleaned_count += 1
                        elif txt_file.stat().st_size < 100:  # Fichiers tr√®s petits
                            with open(txt_file, 'r', encoding='utf-8') as f:
                                content = f.read().strip()
                                if not content or len(content) < 50:
                                    print(f"üóëÔ∏è  Suppression fichier trop petit: {txt_file}")
                                    txt_file.unlink()
                                    cleaned_count += 1
                    except Exception as e:
                        print(f"‚ùå Erreur lors du nettoyage de {txt_file}: {e}")
        
        print(f"üßπ Nettoyage termin√©: {cleaned_count} fichiers supprim√©s")
        return cleaned_count

def load_urls_from_csv(filepath='urls_to_extract.csv'):
    """Charge les URLs et leurs cat√©gories depuis un fichier CSV."""
    urls_by_category = {}
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader) # Skip header row
            for row in reader:
                if len(row) >= 2:
                    category = row[0].strip()
                    url = row[1].strip()
                    if category not in urls_by_category:
                        urls_by_category[category] = []
                    urls_by_category[category].append(url)
    except FileNotFoundError:
        print(f"‚ùå Erreur: Le fichier CSV '{filepath}' n'a pas √©t√© trouv√©.")
        return None
    except Exception as e:
        print(f"‚ùå Erreur lors de la lecture du CSV '{filepath}': {e}")
        return None
    return urls_by_category

def main():
    print("D√©but de l'ex√©cution de verify_extraction.py") # Nouveau log
    
    # Charger les URLs depuis le fichier CSV
    urls_to_extract = load_urls_from_csv('urls_to_extract.csv')
    
    if urls_to_extract is None:
        print("Impossible de continuer sans la liste des URLs √† extraire.")
        return

    # Cr√©er le v√©rificateur
    verifier = ExtractionVerifier()
    print(f"R√©pertoire de sortie configur√©: {verifier.output_dir}") # Nouveau log
    
    # Scanner les fichiers existants
    verifier.scan_existing_files()
    
    # Nettoyer les fichiers vides
    verifier.clean_empty_files()
    
    # V√©rifier la compl√©tude
    stats = verifier.verify_extraction_completeness(urls_to_extract)
    
    # G√©n√©rer le rapport
    verifier.generate_extraction_report(stats)
    
    # Recommandations
    print("\nüí° RECOMMANDATIONS:")
    if stats['missing'] == 0:
        print("‚úÖ Toutes les URLs ont √©t√© extraites avec succ√®s !")
    else:
        print(f"üîÑ {stats['missing']} URLs restent √† traiter")
        print("   Utilisez le script d'extraction pour traiter les URLs manquantes")
    
    print("\nüöÄ COMMANDES SUIVANTES:")
    if stats['missing'] > 0:
        print("   # Extraire toutes les URLs manquantes")
        print("   ./extract_remaining.sh all")
        print()
        print("   # Extraire par cat√©gorie")
        print("   ./extract_remaining.sh priority")

if __name__ == "__main__":
    # Utiliser 'python' pour l'ex√©cution directe
    # Si ce script est appel√© par un autre script shell, il utilisera l'interpr√©teur d√©fini dans ce shell.
    # Pour une ex√©cution directe, 'python' est plus courant sur Windows.
    main()
